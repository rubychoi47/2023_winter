# -*- coding: utf-8 -*-
"""XGBoost

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QKQSaiNm8_448h4iM4tKcdOBBheF1r-c
"""

import xgboost as xgb
import pandas as pd
import numpy as np
import random
import os
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn import datasets
from sklearn.datasets import load_iris
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

import matplotlib.font_manager as fm
fe = fm.FontEntry(fname = 'MaruBuri-Regular.otf', name = 'MaruBuri')

fm.fontManager.ttflist.insert(0, fe)
plt.rc('font', family='MaruBuri')

import warnings
warnings.filterwarnings('ignore')


import matplotlib
from matplotlib import font_manager, rc
import platform
if platform.system()=="Windows":
    font_name=font_manager.FontProperties(fname="c:/Windows/Fonts/malgun.ttf").get_name()
    rc('font', family=font_name)
matplotlib.rcParams['axes.unicode_minus']=False

import warnings
warnings.filterwarnings("ignore")

def seed_everything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)

seed_everything(42)

train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

# 데이터를 확인하기 위해 head() 함수를 사용합니다.
#train.head(5)
test.head(10)

train.info()

test.info()

df=pd.DataFrame(test)
print(df)

# Load iris dataset
iris = load_iris()
x, y = iris.data, iris.target

from sklearn.model_selection import train_test_split

# 데이터를 학습용과 테스트용으로 나눔
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# 나눠진 테스트용 데이터의 실제 타겟 변수를 y_test로 사용

x_train = train.drop(['ID', 'TARGET'], axis = 1)
y_train = train['TARGET']

x_test = test.drop(['ID'], axis = 1)

ordinal_features = ['요일', '범죄발생지']

for feature in ordinal_features:
    le = LabelEncoder()
    le = le.fit(x_train[feature])
    x_train[feature] = le.transform(x_train[feature])

    # x_train데이터에서 존재하지 않았던 값이 x_test 데이터에 존재할 수도 있습니다.
    # 따라서 x_test 데이터를 바로 변형시키지 않고 고윳값을 확인후 x_test 데이터를 변환합니다.
    for label in np.unique(x_test[feature]):
        if label not in le.classes_:
            le.classes_ = np.append(le.classes_, label)
    x_test[feature] = le.transform(x_test[feature])

# Set parameters
params = {
    'objective': 'multi:softmax',
    'num_class': 3,
    'max_depth': 6,
    'learning_rate': 0.05,
    'subsample': 0.9,
    'colsample_bytree': 0.9
}

# 카테고리 변수를 제거한 후 정규화
numeric_columns = x_train.select_dtypes(include=['int', 'float']).columns
X_train_numeric = x_train[numeric_columns]
X_test_numeric = x_test[numeric_columns]
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train_normalized = scaler.fit_transform(X_train_numeric)
X_test_normalized = scaler.transform(X_test_numeric)

#from sklearn.preprocessing import MinMaxScaler

#scaler = MinMaxScaler()
#X_train_normalized = scaler.fit_transform(x_train.values)
#X_test_normalized = scaler.transform(x_test)

# 데이터프레임 로드
data = pd.read_csv('train.csv')

# 카테고리형 열을 카테고리로 변환
data['요일'] = data['요일'].astype('category')
data['범죄발생지'] = data['범죄발생지'].astype('category')


# 특성 변수를 DMatrix로 변환
dtrain = xgb.DMatrix(data=x, label=y, enable_categorical=True)

# 'TEST' 열을 제외한 나머지 특성을 사용하여 x_train, x_test 생성
x_train = train.drop(['ID', 'TARGET'], axis=1)
x_test = test.drop(['ID'], axis=1)

x_train['요일'] = x_train['요일'].astype('category')
x_train['범죄발생지'] = x_train['범죄발생지'].astype('category')

x_test['요일'] = x_test['요일'].astype('category')
x_test['범죄발생지'] = x_test['범죄발생지'].astype('category')

from sklearn.preprocessing import LabelEncoder

# LabelEncoder 객체 생성
label_encoder = LabelEncoder()

# 타겟 변수를 숫자로 변환
y_test_encoded = label_encoder.fit_transform(y_test)

# 변환된 타겟 변수를 xgb.DMatrix로 전달하여 생성
xgb_test = xgb.DMatrix(data=x_test, enable_categorical=True)

#xgb_test = xgb.DMatrix(data=x_test, label=y_test_encoded, enable_categorical=True)

# 특성 변수를 DMatrix로 변환할 때 enable_categorical=True 설정
xgb_train = xgb.DMatrix(x_train, label=y_train, enable_categorical=True)

# 모델 학습
model = xgb.train(params, xgb_train)


# 테스트 데이터셋에 대한 예측
y_pred = model.predict(xgb_test)
# 테스트 데이터셋에 대한 예측값을 DMatrix로 변환
#xgb_test = xgb.DMatrix(x_test, label=y_pred, enable_categorical=True)

model = xgb.train(params, xgb_train)

y_pred = model.predict(xgb_test)

y_pred = [int(round(x)) for x in y_pred]

print("테스트 데이터셋의 길이:", len(y_test))
print("테스트 데이터셋의 길이:", len(y_pred))
y_pred = y_pred[:len(y_test)]
print("테스트 데이터셋의 길이:", len(y_test))
print("테스트 데이터셋의 길이:", len(y_pred))

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print('XGBoost Accuracy:', accuracy)

# 범주형 변수로 변환
#test['요일'] = test['요일'].astype('category')
#test['범죄발생지'] = test['범죄발생지'].astype('category')

# 특성 변수를 DMatrix로 변환할 때 enable_categorical=True 설정
#xgb_test = xgb.DMatrix(data=test, enable_categorical=True)

#xgb_test = xgb.DMatrix(data=test, enable_categorical=True)
#y_pred = model.predict(xgb_test)

# 예측된 값(예측된 Target)을 test 데이터에 추가
#test['Predicted_Target'] = y_pred

from xgboost import plot_importance ## Feature Importance를 불러오기 위함
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score
from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score
from xgboost import XGBRegressor
import math
import warnings

#XGBoost 모델의 실제값과 예측값의 비교 시각화
fig = plt.figure( figsize = (12, 4))
chart = fig.add_subplot(1,1,1)
chart.plot(y_test[:200], marker='o', color='blue', label='Actual value')
chart.plot(y_pred[:200], marker='^', color='red', label='Predicted value')
chart.set_title('XGBoost prediction result', size=30)
plt.xlabel('number of times', size=20)
plt.ylabel('Sales', size=20)
plt.legend(loc = 'best')

