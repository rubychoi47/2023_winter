# -*- coding: utf-8 -*-
"""누아보_해커톤.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17T-XmDib67_ojx9kGqR17TEFi8NpBr-r
"""

from google.colab import drive
drive.mount('/content/drive')

#폰트 설치
!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf

!fc-list | grep Nanum

import pandas as pd
import seaborn as sns
import numpy as np

import matplotlib.pyplot as plt
plt.rc('font', family='NanumBarunGothic')
plt.rcParams['axes.unicode_minus'] =False

# test 데이터를 분석에 활용하는 것은 Data Leakage에 해당하므로 train 데이터만 사용합니다.
train = pd.read_csv('/content/drive/MyDrive/csv/train.csv')
train.head()

train.info()

"""train 데이터는 총 84406개 행(row)이 존재합니다. 결측치는 존재하지 않으며 요일, 범죄발생지 피처의 데이터타입이 object임을 알 수 있습니다"""

train.isnull().sum()

# 기술통계량을 확인하기위해 describe() 함수를 사용합니다.
train.describe()

"""##**이상치 확인**
- 데이터 내부에는 평균적인 관측치와 멀리 떨어진 이상치가 존재하는 경우가 있음
- boxplot 사용
"""

plt.rcParams['font.family'] = 'NanumBarunGothic'
fig, axes = plt.subplots(2, 2, figsize=(5, 5))

sns.boxplot(y = train['사건발생거리'], ax = axes[0][0])
sns.boxplot(y = train['강수량(mm)'], ax = axes[0][1])

sns.boxplot(y = train['강설량(mm)'], ax = axes[1][0])
sns.boxplot(y = train['적설량(cm)'], ax = axes[1][1])

plt.show()

#이상치계산
def outlier_iqr(data, column):

    # lower, upper 글로벌 변수 선언하기
    global lower, upper

    # 4분위수 기준 지정하기
    q25, q75 = np.quantile(data[column], 0.25), np.quantile(data[column], 0.75)

    # IQR 계산하기
    iqr = q75 - q25

    # outlier cutoff 계산하기
    cut_off = iqr * 1.5

    # lower와 upper bound 값 구하기
    lower, upper = q25 - cut_off, q75 + cut_off

    print('IQR은',iqr, '이다.')
    print('lower bound 값은', lower, '이다.')
    print('upper bound 값은', upper, '이다.')

    # 1사 분위와 4사 분위에 속해있는 데이터 각각 저장하기
    data1 = data[data[column] > upper]
    data2 = data[data[column] < lower]

    # 이상치 총 개수 구하기
    return print('총 이상치 개수는', data1.shape[0] + data2.shape[0], '이다.\n')

# 사건발생거리 이상치 계산 및 제거
outlier_iqr(train,'사건발생거리')
train_cleaned = train[(train['사건발생거리'] >= lower) & (train['사건발생거리'] <= upper)]

# 강수량(mm) 이상치 계산 및 제거
outlier_iqr(train,'강수량(mm)')
train_cleaned = train[(train['강수량(mm)'] >= lower) & (train['강수량(mm)'] <= upper)]

#강설량(mm) 이상치 제거
outlier_iqr(train,'강설량(mm)')
train_cleaned = train[(train['강설량(mm)'] >= lower) & (train['강설량(mm)'] <= upper)]

#적설량(cm) 이상치 제거
outlier_iqr(train,'적설량(cm)')
train_cleaned = train[(train['적설량(cm)'] >= lower) & (train['적설량(cm)'] <= upper)]

#결측치 제거 확인
len(train),len(train_cleaned)

"""## **상관관계 확인**
- feature간의 상관관계를 확인함으로써 종속변수와 연관된 변수, 다중공선성이 일어날 변수 등을 확인할 수 있습니다.
- 머신러닝에서 피처들 간에 상관관계가 높아서 발생하는 문제를 다중공선성이라 합니다.
- 다중공선성이 존재하는 변수들을 제거하지 않을 경우 모델의 예측 성능이 저하될 수 있습니다.
"""

# 상관계수를 구글링 해서 찍어봅시다!
train_cleaned = train_cleaned.drop(['ID'], axis = 1)
print(train_cleaned.corr(method='pearson'))

plt.figure(figsize = (15,15), dpi = 100)
sns.heatmap(train_cleaned.corr(), annot = True, cmap = 'Blues')
plt.show()
#train_cleaned.isnull().sum()

"""##**모델 학습시키기**"""

train=train_cleaned
test = pd.read_csv('/content/drive/MyDrive/csv/test.csv')
print(train_cleaned.columns)
print(test.columns)

x_train = train_cleaned.drop(columns="TARGET")
y_train = train_cleaned['TARGET']

cols = ["요일", "범죄발생지"]

for i in range(len(cols)):
    print("\n--- 종류: ", x_train[cols[i]].unique().size)
    print(x_train[cols[i]].value_counts())

!pip install catboost
import catboost

!pip install optuna
import optuna

# 범주형 변수(object) 레이블 인코딩 적용

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from catboost import CatBoostClassifier
import optuna
from optuna import Trial, visualization
from optuna.samplers import TPESampler

# 데이터 분리
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.3, stratify=y_train, random_state=316)

print(x_train.shape, y_train.shape)
print(x_val.shape, y_val.shape)

# train, val 데이터 세트의 타겟별 분포 비율 확인 --> 불균형

print(y_train.value_counts(normalize=True))
print(y_val.value_counts(normalize=True))

encoder = LabelEncoder()

for i in range(len(cols)):
    x_train[cols[i]] = encoder.fit_transform(x_train[cols[i]])
    test[cols[i]] = encoder.transform(test[cols[i]])

# 클래스별 가중치 계산
import numpy as np

weight = dict()

n_classes = 3
cnt = np.bincount(y_train)
n_samples = cnt.sum()

for i in range(n_classes):
    weight[i] =  n_samples / (n_classes * cnt[i])

print(weight)

def objectiveCAT(trial: Trial, x_tr, y_tr, x_val, y_val, weight):
    param = {
        'iterations' : trial.suggest_int('iterations', 100, 1000),
        'depth' : trial.suggest_int('depth', 3, 5,8),
        'learning_rate' : trial.suggest_float('learning_rate', 0.001, 0.1),
        'random_state' : 0,
        'class_weights': weight,
        'cat_features' : cat_cols
    }

    # 학습 모델 생성
    model = CatBoostClassifier(**param)
    cat_model = model.fit(x_tr, y_tr, verbose=True)

    # 모델 성능 확인
    pred = cat_model.predict(x_val)
    score = f1_score(y_val, pred, average="micro")

    return score

cat_cols = np.where(x_train.dtypes != float)[0]
cat_cols

# 하이퍼 파라미터 튜닝
from sklearn.metrics import f1_score

study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=0))
study.optimize(lambda trial : objectiveCAT(trial, x_train, y_train, x_val, y_val, weight), n_trials = 30)

print('Best trial : score {}, \nparams {}'.format(study.best_trial.value, study.best_trial.params))

# 최적의 하이퍼 파라미터 저장

params = study.best_trial.params
params["random_state"] = 0
params["class_weights"] = weight
params["cat_features"] = cat_cols
params

# 최종 모델 학습

model = CatBoostClassifier(**params)
model.fit(x_train, y_train, verbose=True)

# 변수중요도 확인
plt.barh(x_train.columns, model.feature_importances_)

from sklearn.metrics import classification_report, accuracy_score

#훈련데이터에 대한 예측
train_pred=model.predict(x_val)


# 테스트 데이터에 대한 예측
test_pred = model.predict(x_val)

# 훈련 모델 성능 평가
train_accuracy = accuracy_score(y_val, train_pred)
train_classification_rep = classification_report(y_val, train_pred)

# 테스트 모델 성능 평가
test_accuracy = accuracy_score(y_val, test_pred)
test_classification_rep = classification_report(y_val, test_pred)

print("훈련 데이터")
print(train_accuracy)
print(train_classification_rep)

print("테스트 데이터")
print(test_accuracy)
print(test_classification_rep)

selected_features = ['범죄발생지', '소관지역', '소관경찰서','시간','요일','월','TARGET']
# 선택한 열만 있는 새로운 데이터프레임 생성
selected_data = train_cleaned[selected_features]
selected_data

selected_data.info()
selected_data.isnull().sum()

x_train = selected_data.drop(columns="TARGET")
y_train = selected_data['TARGET']

x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.3, stratify=y_train, random_state=316)

# train, val 데이터 세트의 타겟별 분포 비율 확인 --> 불균형

print(y_train.value_counts(normalize=True))
print(y_val.value_counts(normalize=True))

selected_test_featured=['범죄발생지', '소관지역', '소관경찰서','시간','요일','월']
# 선택한 열만 있는 새로운 데이터프레임 생성
selected_test_data = test[selected_test_featured]
selected_test_data.head(6)

from sklearn.preprocessing import LabelEncoder

# 레이블 인코더 생성
label_encoder = LabelEncoder()

# 각 열에 대해 레이블 인코딩 적용
for feature in ['범죄발생지', '소관지역', '소관경찰서', '요일']:
    selected_data[feature] = label_encoder.fit_transform(selected_data[feature])

# 결과 확인
selected_data

# 클래스별 가중치 계산
import numpy as np

weight = dict()

n_classes = 3
cnt = np.bincount(y_train)
n_samples = cnt.sum()

for i in range(n_classes):
    weight[i] =  n_samples / (n_classes * cnt[i])

print(weight)

import numpy as np

# x_train DataFrame에서 실수형이 아닌 열의 인덱스를 찾음
cat_cols = np.where(x_train.dtypes != float)[0]

# cat_cols 변수를 출력하여 확인
print(cat_cols)

from catboost import CatBoostClassifier
from sklearn.metrics import f1_score
from optuna import Trial

def objectiveCAT(trial: Trial, x_tr, y_tr, x_val, y_val, weight, cat_cols):
    param = {
        'iterations': trial.suggest_int('iterations', 100, 1000),
        'depth': trial.suggest_int('depth', 3, 8),
        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),
        'random_state': 0,
        'class_weights': weight,
        'cat_features': cat_cols
    }

    # 학습 모델 생성
    model = CatBoostClassifier(**param)
    cat_model = model.fit(x_tr, y_tr, verbose=True)

    # 모델 성능 확인
    pred = cat_model.predict(x_val)
    score = f1_score(y_val, pred, average="micro")

    return score

# 데이터프레임의 모든 열에 대해 실수형 값을 문자열로 변환
x_train = x_train.astype(str)

# 모든 열에 대해 NaN 값을 문자열 "NaN"으로 변환
x_train.fillna("NaN", inplace=True)

# 하이퍼 파라미터 튜닝
from sklearn.metrics import f1_score

study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=0))
study.optimize(lambda trial : objectiveCAT(trial, x_train, y_train, x_val, y_val, weight, cat_cols), n_trials = 30)

print('Best trial : score {}, \nparams {}'.format(study.best_trial.value, study.best_trial.params))

# 최적의 하이퍼 파라미터 저장

params = study.best_trial.params
params["random_state"] = 0
params["class_weights"] = weight
params["cat_features"] = cat_cols
params

# 최종 모델 학습
final_model = CatBoostClassifier(**params)
final_model.fit(x_train, y_train, verbose=True)

#훈련데이터에 대한 예측
train_pred=final_model.predict(x_val)

# 테스트 데이터에 대한 예측
test_pred = final_model.predict(x_val)

# 훈련 모델 성능 평가
train_accuracy = accuracy_score(y_val, train_pred)
train_classification_rep = classification_report(y_val, train_pred)

# 테스트 모델 성능 평가
test_accuracy = accuracy_score(y_val, test_pred)
test_classification_rep = classification_report(y_val, test_pred)

print("훈련 데이터")
print(train_accuracy)
print(train_classification_rep)

print("테스트 데이터")
print(test_accuracy)
print(test_classification_rep)

