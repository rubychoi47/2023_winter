# -*- coding: utf-8 -*-
"""cyj

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w-fd9wzwIVXiSvNbSHGGZFPqFr5gfBk3
"""

pip install pycaret

import os
import random
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import GridSearchCV
import lightgbm as lgb
from pycaret.classification import *
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris

"""시드 고정"""

def seed_everything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)

seed_everything(42) # Seed 고정

"""## Load data"""

train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
train.head()

"""## Label encoding"""

train.drop('ID', axis = 1, inplace=True)
test.drop('ID', axis = 1, inplace=True)
ordinal_features = ['요일', '범죄발생지']
for feature in ordinal_features:
    le = LabelEncoder()
    le = le.fit(train[feature])
    train[feature] = le.transform(train[feature])

    for label in np.unique(test[feature]):
        if label not in le.classes_:
            le.classes_ = np.append(le.classes_, label)
    test[feature] = le.transform(test[feature])

"""## pycaret

ML 모델 간의 성능을 비교하기 위함
"""

train.dropna(subset=['TARGET'], inplace=True)

clf = setup(data=train,
            target='TARGET',
            normalize=True,
            # polynomial_features=True,
            session_id=42,
            use_gpu =True
            )

top3 = compare_models(sort='f1', n_select=5, fold=5)

result = pull()
display(result)

import xgboost as xgb
import pandas as pd
import numpy as np
import random
import os
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn import datasets
from sklearn.datasets import load_iris
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

from sklearn.metrics import accuracy_score
from sklearn.ensemble import GradientBoostingClassifier
import matplotlib.font_manager as fm
fe = fm.FontEntry(fname = 'MaruBuri-Regular.otf', name = 'MaruBuri')

fm.fontManager.ttflist.insert(0, fe)
plt.rc('font', family='MaruBuri')

import warnings
warnings.filterwarnings('ignore')


import matplotlib
from matplotlib import font_manager, rc
import platform
if platform.system()=="Windows":
    font_name=font_manager.FontProperties(fname="c:/Windows/Fonts/malgun.ttf").get_name()
    rc('font', family=font_name)
matplotlib.rcParams['axes.unicode_minus']=False

import warnings
warnings.filterwarnings("ignore")

def seed_everything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)

seed_everything(42)

train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

# 데이터를 확인하기 위해 head() 함수를 사용합니다.
#train.head(5)
test.head(10)

print(sorted(train['범죄발생지'].unique()))

# Load iris dataset
iris = load_iris()
x, y = iris.data, iris.target

# 데이터를 학습용과 테스트용으로 나눔
yX_train, X_test, y_train, _test = train_test_split(x, y, test_size=0.2, random_state=42)

# 나눠진 테스트용 데이터의 실제 타겟 변수를 y_test로 사용
x_train = train.drop(['ID', 'TARGET'], axis = 1)
y_train = train['TARGET']

x_test = test.drop(['ID'], axis = 1)

ordinal_features = ['요일', '범죄발생지']

for feature in ordinal_features:
    le = LabelEncoder()
    le = le.fit(x_train[feature])
    x_train[feature] = le.transform(x_train[feature])

    # x_train데이터에서 존재하지 않았던 값이 x_test 데이터에 존재할 수도 있습니다.
    # 따라서 x_test 데이터를 바로 변형시키지 않고 고윳값을 확인후 x_test 데이터를 변환합니다.
    for label in np.unique(x_test[feature]):
        if label not in le.classes_:
            le.classes_ = np.append(le.classes_, label)
    x_test[feature] = le.transform(x_test[feature])

gbc = GradientBoostingClassifier(random_state=0) # 기본값: max_depth=3, learning_rate=0.1

gbc.fit(x_train, y_train)



score_train = gbc.score(x_train, y_train) # train set 정확도

print('{:.3f}'.format(score_train))

x_test = x_test[:len(y_test)]

score_test = gbc.score(x_test, y_test) # 일반화 정확도

print('{:.3f}'.format(score_test))